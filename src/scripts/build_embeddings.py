# src/scripts/build_embeddings.py
"""
–ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings –¥–ª—è –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤ –≤ products.db.

–ü—Ä–æ—Ü–µ—Å—Å:
1. –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å SentenceTransformer
2. –ß–∏—Ç–∞–µ—Ç –≤—Å–µ —Ç–æ–≤–∞—Ä—ã –ë–ï–ó embeddings
3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç embeddings –±–∞—Ç—á–∞–º–∏
4. –û–±–Ω–æ–≤–ª—è–µ—Ç –∫–æ–ª–æ–Ω–∫—É embedding –≤ –ë–î

–ó–∞–ø—É—Å–∫:
    # –í—Å–µ —Ç–æ–≤–∞—Ä—ã –±–µ–∑ embeddings
    uv run python -m src.scripts.build_embeddings
    
    # –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –≤—Å–µ embeddings (–≤–∫–ª—é—á–∞—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ)
    uv run python -m src.scripts.build_embeddings --rebuild
    
    # –¢–æ–ª—å–∫–æ mock —Ç–æ–≤–∞—Ä—ã
    uv run python -m src.scripts.build_embeddings --mocks-only
"""

import argparse
import numpy as np
import torch
from pathlib import Path
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
from typing import List, Tuple

# ==================== –ò–ú–ü–û–†–¢–´ ====================
from src.utils.queries import get_connection, DB_PATH


# ==================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ====================

MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
BATCH_SIZE = 512 


# ==================== –§–£–ù–ö–¶–ò–ò ====================

def get_device() -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ."""
    if torch.backends.mps.is_available():
        return "mps"
    elif torch.cuda.is_available():
        return "cuda"
    else:
        return "cpu"


def load_model(device: str) -> SentenceTransformer:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å SentenceTransformer."""
    print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {MODEL_NAME} –Ω–∞ {device.upper()}...")
    model = SentenceTransformer(MODEL_NAME, device=device)
    embedding_dim = model.get_sentence_embedding_dimension()
    print(f"   ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {embedding_dim})")
    return model


def fetch_products_without_embeddings(mocks_only: bool = False) -> List[Tuple]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Ç–æ–≤–∞—Ä—ã –±–µ–∑ embeddings.
    
    Args:
        mocks_only: –¢–æ–ª—å–∫–æ mock —Ç–æ–≤–∞—Ä—ã (id >= 900000)
    
    Returns:
        List[Tuple]: [(id, product_name, product_category, brand), ...]
    """
    conn = get_connection()
    cursor = conn.cursor()
    
    query = """
        SELECT id, product_name, product_category, brand
        FROM products
        WHERE embedding IS NULL
    """
    
    if mocks_only:
        query += " AND id >= 900000"
    
    query += " ORDER BY id"
    
    cursor.execute(query)
    products = cursor.fetchall()
    conn.close()
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º Row –≤ tuple
    return [(row['id'], row['product_name'], row['product_category'], row['brand']) 
            for row in products]


def create_embedding_text(product_name: str, product_category: str, brand: str) -> str:
    """
    –°–æ–∑–¥–∞—ë—Ç —Ç–µ–∫—Å—Ç –¥–ª—è embedding.
    
    –§–æ—Ä–º–∞—Ç: "–ù–∞–∑–≤–∞–Ω–∏–µ –ö–∞—Ç–µ–≥–æ—Ä–∏—è –ë—Ä–µ–Ω–¥"
    """
    name = str(product_name).strip() if product_name else ""
    category = str(product_category).strip() if product_category else ""
    brand_str = str(brand).strip() if brand else ""
    
    text = f"{name} {category} {brand_str}".strip()
    return text


def save_embeddings_batch(product_ids: List[int], embeddings: np.ndarray):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –±–∞—Ç—á embeddings –≤ –ë–î."""
    conn = get_connection()
    cursor = conn.cursor()
    
    data = []
    for product_id, embedding in zip(product_ids, embeddings):
        embedding_bytes = embedding.astype(np.float32).tobytes()
        data.append((embedding_bytes, product_id))
    
    cursor.executemany("""
        UPDATE products
        SET embedding = ?
        WHERE id = ?
    """, data)
    
    conn.commit()
    conn.close()


def rebuild_all_embeddings():
    """–£–¥–∞–ª—è–µ—Ç –≤—Å–µ embeddings –¥–ª—è –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è."""
    print("\nüóëÔ∏è  –£–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö embeddings...")
    
    conn = get_connection()
    cursor = conn.cursor()
    
    cursor.execute("UPDATE products SET embedding = NULL")
    conn.commit()
    
    cursor.execute("SELECT COUNT(*) FROM products")
    total = cursor.fetchone()[0]
    
    conn.close()
    
    print(f"   ‚úÖ –û—á–∏—â–µ–Ω–æ embeddings –¥–ª—è {total:,} —Ç–æ–≤–∞—Ä–æ–≤")


def build_embeddings(mocks_only: bool = False, rebuild: bool = False):
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embeddings.
    
    Args:
        mocks_only: –¢–æ–ª—å–∫–æ mock —Ç–æ–≤–∞—Ä—ã
        rebuild: –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –≤—Å–µ embeddings
    """
    print("=" * 70)
    print("üß† –ì–ï–ù–ï–†–ê–¶–ò–Ø EMBEDDINGS")
    print("=" * 70)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ë–î
    if not DB_PATH.exists():
        print(f"‚ùå –ë–î –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {DB_PATH}")
        print("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ: uv run python -m src.scripts.prepare_db")
        return
    
    # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ
    if rebuild:
        rebuild_all_embeddings()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    device = get_device()
    print(f"üñ•Ô∏è  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device.upper()}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    model = load_model(device)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–≤–∞—Ä—ã
    print(f"\nüìö –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤...")
    products = fetch_products_without_embeddings(mocks_only=mocks_only)
    
    if not products:
        print("‚úÖ –í—Å–µ —Ç–æ–≤–∞—Ä—ã —É–∂–µ –∏–º–µ—é—Ç embeddings!")
        return
    
    total = len(products)
    print(f"   –ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤ –±–µ–∑ embeddings: {total:,}")
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ –±–∞—Ç—á–∞–º
    print(f"\nüîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings (batch_size={BATCH_SIZE})...")
    
    num_batches = (total + BATCH_SIZE - 1) // BATCH_SIZE
    
    for batch_idx in tqdm(range(num_batches), desc="–ë–∞—Ç—á–∏"):
        start_idx = batch_idx * BATCH_SIZE
        end_idx = min(start_idx + BATCH_SIZE, total)
        batch_products = products[start_idx:end_idx]
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç—ã
        batch_texts = [
            create_embedding_text(name, category, brand)
            for _, name, category, brand in batch_products
        ]
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embeddings
        batch_embeddings = model.encode(
            batch_texts,
            convert_to_numpy=True,
            show_progress_bar=False,
            batch_size=BATCH_SIZE
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        batch_ids = [product_id for product_id, _, _, _ in batch_products]
        save_embeddings_batch(batch_ids, batch_embeddings)
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "=" * 70)
    print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("=" * 70)
    
    conn = get_connection()
    cursor = conn.cursor()
    
    cursor.execute("SELECT COUNT(*) FROM products WHERE embedding IS NOT NULL")
    with_embeddings = cursor.fetchone()[0]
    
    cursor.execute("SELECT COUNT(*) FROM products")
    total_products = cursor.fetchone()[0]
    
    conn.close()
    
    embedding_dim = model.get_sentence_embedding_dimension()
    
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {total:,}")
    print(f"–¢–æ–≤–∞—Ä–æ–≤ —Å embeddings: {with_embeddings:,} / {total_products:,}")
    print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {embedding_dim}")
    print(f"–†–∞–∑–º–µ—Ä –æ–¥–Ω–æ–≥–æ embedding: {embedding_dim * 4 / 1024:.2f} KB")
    print(f"–û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {with_embeddings * embedding_dim * 4 / 1024 / 1024:.2f} MB")
    print("=" * 70)
    print("‚úÖ EMBEDDINGS –°–û–ó–î–ê–ù–´")
    print("=" * 70)


# ==================== MAIN ====================

def main():
    """CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å."""
    parser = argparse.ArgumentParser(description='–ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings –¥–ª—è products.db')
    parser.add_argument(
        '--mocks-only',
        action='store_true',
        help='–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –¥–ª—è mock —Ç–æ–≤–∞—Ä–æ–≤'
    )
    parser.add_argument(
        '--rebuild',
        action='store_true',
        help='–ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –≤—Å–µ embeddings (—É–¥–∞–ª–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ)'
    )
    
    args = parser.parse_args()
    
    build_embeddings(mocks_only=args.mocks_only, rebuild=args.rebuild)


if __name__ == "__main__":
    main()

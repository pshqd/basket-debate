# src/rl/train_shared_ppo.py
"""
–ü—Ä–æ—Å—Ç–æ–π —É—á–µ–±–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω: –æ–±—É—á–∞–µ–º shared PPO –ø–æ–ª–∏—Ç–∏–∫—É –¥–ª—è –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤.

–ü–∞–π–ø–ª–∞–π–Ω:
1. –°–æ–∑–¥–∞—ë–º —Å—Ä–µ–¥—É —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º K=100 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
2. –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—ë—Ä—Ç–∫–∏ SuperSuit (pad_action_space, pad_observations)
3. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º PettingZoo ‚Üí VectorEnv
4. –û–±—É—á–∞–µ–º PPO –Ω–∞ 50k —à–∞–≥–æ–≤
5. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
"""

import supersuit as ss
from stable_baselines3 import PPO
from stable_baselines3.ppo import MlpPolicy
from src.backend.db.queries import fetch_candidate_products
from src.agent.env import create_basket_env
from src.agent.utils import pad_products_to_k

# –ö–û–ù–°–¢–ê–ù–¢–ê: —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
K = 100


def make_env(seed: int = 0):
    """
    –°–æ–∑–¥–∞—ë—Ç –æ–∫—Ä—É–∂–µ–Ω–∏–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
    
    –ö–ª—é—á–µ–≤–∞—è –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å: —Ñ–∏–∫—Å–∏—Ä—É–µ–º K=100 —Ç–æ–≤–∞—Ä–æ–≤, —á—Ç–æ–±—ã action_space –±—ã–ª —Å—Ç–∞–±–∏–ª—å–Ω—ã–º.
    """
    # 1. –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ constraints –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    constraints = {
        "budget_rub": 1500,
        "exclude_tags": ["dairy"],
        "include_tags": [],
        "meal_type": ["dinner"],
        "people": 3,
    }
    
    # 2. –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–≤–∞—Ä—ã –∏–∑ –ë–î
    products = fetch_candidate_products(constraints, limit=K)
    print(f"[INFO] Fetched {len(products)} products from DB")
    
    # 3. –í–ê–ñ–ù–û: –ü–∞–¥–¥–∏–º –¥–æ K (–¥–æ–±–∞–≤–ª—è–µ–º dummy items, –µ—Å–ª–∏ –º–µ–Ω—å—à–µ)
    products = pad_products_to_k(products, k=K)
    print(f"[INFO] Padded to {len(products)} products (K={K})")
    
    # 4. –°–æ–∑–¥–∞—ë–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ
    env = create_basket_env(
        products=products,
        constraints=constraints,
        max_steps=10,
        render_mode=None  # <-- –í–ê–ñ–ù–û: –ø–µ—Ä–µ–¥–∞—ë–º render_mode
    )
    
    # 5. –û–±—ë—Ä—Ç–∫–∏ SuperSuit (–ø–∞–¥–¥–∏–Ω–≥ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤)
    # pad_action_space: –ø—Ä–∏–≤–æ–¥–∏—Ç action space –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤ –∫ –æ–¥–Ω–æ–º—É —Ä–∞–∑–º–µ—Ä—É
    env = ss.pad_action_space_v0(env)
    
    # pad_observations: –ø—Ä–∏–≤–æ–¥–∏—Ç observation space –∫ –æ–¥–Ω–æ–º—É —Ä–∞–∑–º–µ—Ä—É
    env = ss.pad_observations_v0(env)
    
    # 6. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º PettingZoo ParallelEnv ‚Üí VectorEnv –¥–ª—è SB3
    env = ss.pettingzoo_env_to_vec_env_v1(env)
    
    # 7. –û–±—ë—Ä—Ç–∫–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å SB3
    env = ss.concat_vec_envs_v1(env, 1, num_cpus=0, base_class="stable_baselines3")
    
    return env


if __name__ == "__main__":
    print("=" * 60)
    print("–ü—Ä–æ—Å—Ç–æ–π —É—á–µ–±–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω: Shared PPO –¥–ª—è BasketEnv")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞—ë–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ
    print("\n[1/4] –°–æ–∑–¥–∞—ë–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ...")
    env = make_env(seed=0)
    print(f"‚úÖ –û–∫—Ä—É–∂–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ: action_space={env.action_space}, obs_space={env.observation_space}")
    
    # –°–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å PPO (shared policy –¥–ª—è –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤)
    print("\n[2/4] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º PPO...")
    model = PPO(
        "MlpPolicy",
        env,
        verbose=1,
        n_steps=2048,        # –ë–´–õ–û 1024 ‚Üí —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º (–±–æ–ª—å—à–µ exploration)
        batch_size=128,      # –ë–´–õ–û 256 ‚Üí —É–º–µ–Ω—å—à–∞–µ–º (–±–æ–ª–µ–µ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è)
        learning_rate=1e-4,  # –ë–´–õ–û 3e-4 ‚Üí —É–º–µ–Ω—å—à–∞–µ–º (–±–æ–ª–µ–µ –º–µ–¥–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ)
        tensorboard_log="./logs/ppo_basket/",
        ent_coef=2.0,        # –ë–´–õ–û 1.5 ‚Üí —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ï–©–Å –ë–û–õ–¨–®–ï
        max_grad_norm=0.5,
        vf_coef=0.25,        # –ë–´–õ–û 0.5 ‚Üí —É–º–µ–Ω—å—à–∞–µ–º –≤–ª–∏—è–Ω–∏–µ critic
        gamma=0.95,
        gae_lambda=0.9,      # –ù–û–í–û–ï: —É–º–µ–Ω—å—à–∞–µ–º (–±–æ–ª—å—à–µ bias, –º–µ–Ω—å—à–µ variance)
        clip_range=0.3,      # –ë–´–õ–û 0.2 ‚Üí —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º (–±–æ–ª—å—à–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π –∑–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ)
        target_kl=0.03       # –ù–û–í–û–ï: –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º KL divergence (—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å)
    )
    print("‚úÖ PPO —Å–æ–∑–¥–∞–Ω")
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    print("\n[3/4] –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å (50k —à–∞–≥–æ–≤)...")
    model.learn(total_timesteps=200_000)
    print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    print("\n[4/4] –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å...")
    model.save("models/ppo_shared_v0")
    print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ models/ppo_shared_v0.zip")
    
    env.close()
    print("\n" + "=" * 60)
    print("üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –ò—Å–ø–æ–ª—å–∑—É–π –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ model.load()")
    print("=" * 60)

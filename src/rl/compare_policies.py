# src/rl/compare_policies.py
"""
–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ —Å random baseline.
"""

from stable_baselines3 import PPO
from src.rl.train_shared_ppo import make_env
import numpy as np

def test_random_policy(n_episodes: int = 10):
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è (baseline)."""
    env = make_env(seed=42)
    rewards = []
    
    for _ in range(n_episodes):
        obs = env.reset()
        episode_reward = 0
        dones = [False]
        
        while not all(dones):
            # –°–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ (–∫–∞–∫ —Ä–∞–Ω—å—à–µ)
            action = [env.action_space.sample() for _ in range(env.num_envs)]
            obs, rew, dones, info = env.step(action)
            episode_reward += rew.sum() if hasattr(rew, 'sum') else sum(rew)
        
        rewards.append(episode_reward)
    
    env.close()
    return np.mean(rewards)

def test_trained_policy(model_path: str, n_episodes: int = 10):
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –ø–æ–ª–∏—Ç–∏–∫—É."""
    model = PPO.load(model_path)
    env = make_env(seed=42)
    rewards = []
    
    for _ in range(n_episodes):
        obs = env.reset()
        episode_reward = 0
        dones = [False]
        
        while not all(dones):
            action, _ = model.predict(obs, deterministic=True)
            obs, rew, dones, info = env.step(action)
            episode_reward += rew.sum() if hasattr(rew, 'sum') else sum(rew)
        
        rewards.append(episode_reward)
    
    env.close()
    return np.mean(rewards)

if __name__ == "__main__":
    print("üîç –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–ª–∏—Ç–∏–∫:\n")
    
    print("1Ô∏è‚É£  Random baseline...")
    random_reward = test_random_policy(n_episodes=20)
    print(f"   ‚Üí –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {random_reward:.2f}\n")
    
    print("2Ô∏è‚É£  –û–±—É—á–µ–Ω–Ω–∞—è PPO...")
    trained_reward = test_trained_policy("models/ppo_shared_v0.zip", n_episodes=20)
    print(f"   ‚Üí –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {trained_reward:.2f}\n")
    
    improvement = ((trained_reward - random_reward) / abs(random_reward)) * 100
    
    print("=" * 60)
    print(f"üìà –£–ª—É—á—à–µ–Ω–∏–µ: {improvement:+.1f}%")
    if improvement > 50:
        print("   ‚úÖ –û–±—É—á–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ!")
    elif improvement > 20:
        print("   ‚ö†Ô∏è  –û–±—É—á–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–æ –µ—Å—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª")
    else:
        print("   ‚ùå –û–±—É—á–µ–Ω–∏–µ –ø–æ—á—Ç–∏ –Ω–µ –ø–æ–º–æ–≥–ª–æ")
    print("=" * 60)

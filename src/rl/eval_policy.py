"""
–û—Ü–µ–Ω–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏: –∑–∞–ø—É—Å–∫–∞–µ–º 10 —ç–ø–∏–∑–æ–¥–æ–≤ –∏ —Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.
"""

from stable_baselines3 import PPO
from src.rl.train_shared_ppo import make_env

def evaluate_policy(model_path: str, n_episodes: int = 10):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –ø–æ–ª–∏—Ç–∏–∫—É –∏ –≤—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É."""
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    model = PPO.load(model_path)
    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path}")
    
    # –°–æ–∑–¥–∞—ë–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ (—Ç–æ –∂–µ, —á—Ç–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è)
    env = make_env(seed=42)
    
    total_rewards = []
    
    for episode in range(n_episodes):
        obs = env.reset()
        episode_reward = 0
        dones = [False]  # <-- –¢–µ–ø–µ—Ä—å —Å–ø–∏—Å–æ–∫/–º–∞—Å—Å–∏–≤
        
        while not all(dones):  # <-- –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –í–°–ï –∞–≥–µ–Ω—Ç—ã –∑–∞–≤–µ—Ä—à–∏–ª–∏
            action, _states = model.predict(obs, deterministic=True)
            obs, rewards, dones, info = env.step(action)  # <-- rewards –∏ dones –≤–æ –º–Ω–æ–∂. —á–∏—Å–ª–µ
            
            # –°—É–º–º–∏—Ä—É–µ–º –Ω–∞–≥—Ä–∞–¥—ã –æ—Ç –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤ (vectorized env –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–∞—Å—Å–∏–≤)
            episode_reward += rewards.sum() if hasattr(rewards, 'sum') else sum(rewards)

        
        total_rewards.append(episode_reward)
        print(f"–≠–ø–∏–∑–æ–¥ {episode+1}: reward = {episode_reward:.2f}")
    
    avg_reward = sum(total_rewards) / len(total_rewards)
    print(f"\nüìä –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ {n_episodes} —ç–ø–∏–∑–æ–¥–æ–≤: {avg_reward:.2f}")
    
    env.close()

if __name__ == "__main__":
    evaluate_policy("models/ppo_shared_v0.zip", n_episodes=10)
